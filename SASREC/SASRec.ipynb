{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "etC2-FZjeyHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf462de-be1a-4b9f-f6a8-8d617bd57dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/RecommanderSystems')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/RecommanderSystems')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 50, # 300 for ml-1m, 50 for others\n",
        "    d = 40,\n",
        "    batch_size = 128,\n",
        "    num_heads = 1, # default\n",
        "    test_batch_size = 100,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 200,\n",
        "    patience = 20\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "--u0DvNahuJl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "SASRec의 경우 학습을 시킬 때 user 당 item의 시퀀스가 필요"
      ],
      "metadata": {
        "id": "mb6FLTlPtzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_item_sequence_dict = defaultdict(list)\n",
        "\n",
        "u_max = -1\n",
        "i_max = -1\n",
        "\n",
        "data = open(config.data, 'r')\n",
        "for line in data:\n",
        "  u, i = line.strip().split(' ') # (user, item) 조합의 txt 파일 읽기\n",
        "  u = int(u)\n",
        "  i = int(i)\n",
        "  user_item_sequence_dict[u].append(i)\n",
        "  if u > u_max:\n",
        "    u_max = u\n",
        "  if i > i_max:\n",
        "    i_max = i\n",
        "\n",
        "user_seq = {}\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "for u in user_item_sequence_dict:\n",
        "  seq = user_item_sequence_dict[u]\n",
        "  l = len(seq)\n",
        "  user_valid[u] = []\n",
        "  user_test[u] = []\n",
        "  if l < 3:\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "  elif l < config.n:\n",
        "    # padding 필요\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2] # n - 1번째\n",
        "    user_test[u] = padded_seq[-1] # n번째\n",
        "  else:\n",
        "    # l > config.n\n",
        "    padded_seq = seq[-config.n:]\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2]\n",
        "    user_test[u] = padded_seq[-1]\n",
        "  user_seq[u] = padded_seq\n",
        "\n",
        "print(\"u_max\", u_max)\n",
        "print(\"i_max\", i_max)\n",
        "print(len(padded_seq))"
      ],
      "metadata": {
        "id": "uxiK7K70izuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8f4382-e706-42e9-b808-a36d6166cb97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_max 334730\n",
            "i_max 13047\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head"
      ],
      "metadata": {
        "id": "RO8eEmvVYQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, kdim = None, vdim = None):\n",
        "    \"\"\"\n",
        "\n",
        "    embed_dim: Dimensionality of the input and output embeddings.\n",
        "    num_heads: Number of attention heads.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.kdim = kdim if kdim is not None else embed_dim\n",
        "    self.vdim = vdim if vdim is not None else embed_dim\n",
        "\n",
        "    # 그냥 Q, K, V 사이즈 통일\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.kdim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.vdim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    # nn.init.uniform_(self.q_proj.bias)\n",
        "    # nn.init.uniform_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.q_proj.bias)\n",
        "    nn.init.zeros_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.v_proj.bias)\n",
        "    nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Head 늘리는 방식 중에 embedding 사이즈 줄여서 head수 늘리는 방식\n",
        "\n",
        "    x : (batch_size, -1, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "      (batch_size, num_heads, -1, embed_dim // num_heads).\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = x.shape[0]\n",
        "    splited = x.reshape(n_batch, -1, self.num_heads, self.embed_dim // self.num_heads)\n",
        "\n",
        "    return splited.transpose(1, 2) # (B, num_heads, -1, embed_dim // num_heads)\n",
        "\n",
        "  def scaled_dot_product_attention(self, wq, wk, wv, pad_mask = None):\n",
        "    \"\"\"\n",
        "\n",
        "    wq, wk, wv: (B, num_heads, n_seq, embed_dim // num_heads).\n",
        "\n",
        "    Returns:\n",
        "      Scaled Dot-Product Attention  (B, n_seq, embed_dim).\n",
        "      Average attention weights across heads (B, n_seq, n_key) -시각화에 사용\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_batch = wq.shape[0]\n",
        "    d_k = self.embed_dim // self.num_heads\n",
        "\n",
        "    \"\"\"\n",
        "    wq @ wk.T: (B, num_heads, n_seq, n_key)\n",
        "    pad_mask: (B, n_key)\n",
        "    softmax @ v: (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    \"\"\"\n",
        "\n",
        "    wk_t = wk.transpose(2, 3) # (B, num_heads, embed_dim // num_heads, n_key)\n",
        "\n",
        "    similarity = torch.matmul(wq, wk_t) # (B, num_heads, n_seq, n_key)\n",
        "    similarity /= math.sqrt(d_k)\n",
        "    epsilon = -1e9\n",
        "\n",
        "    # pad_mask가 여기서는 tril로 들어와서 (n, n)으로 들어옴 => (1, 1, n, n)으로 unsqueeze\n",
        "    if pad_mask is not None:\n",
        "      similarity = similarity.masked_fill(pad_mask.unsqueeze(0).unsqueeze(1), epsilon) # (B, 1, 1, n_key)\n",
        "\n",
        "    softmax_sim = torch.softmax(similarity, dim = 3) # key마다 얼마나 비슷 -> softmax (B, num_heads, n_seq, n_key)\n",
        "    average_att_w = torch.mean(softmax_sim, dim = 1) # head 마다 (B, n_seq, n_key) 평균\n",
        "\n",
        "    # average_att_w 찍어보면 각 key 마다의 활성화정도 시각화 가능\n",
        "\n",
        "    attention_value = torch.matmul(softmax_sim, wv) # (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    attention_value = attention_value.transpose(1, 2) # (B, n_key, num_heads, embed_dim // num_heads)\n",
        "    # 사이즈 원상 복구 근데 이게 맞나??? 좀 이상함,,\n",
        "    attention_value = attention_value.reshape(n_batch, -1, self.embed_dim) # (B, n_key, embed_dim)\n",
        "\n",
        "    return attention_value, average_att_w\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    \"\"\"\n",
        "    q: (B, n_seq, embed_dim)\n",
        "    k: (B, n_key, kdim)\n",
        "    v: (B, n_key, vdim)\n",
        "    pad_mask: (B, n_key)\n",
        "\n",
        "    Returns:\n",
        "      output (B, -1, embed_dim)\n",
        "      average_att_w: (B, n_seq, n_key)\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Multihead input q\", q.shape)\n",
        "    wq = self.q_proj(q) #(B, n_seq, embed_dim)\n",
        "    wk = self.k_proj(k) #(B, n_key, kdim)\n",
        "    wv = self.v_proj(v) # #(B, n_key, embed_dim)\n",
        "\n",
        "    wk = self.split_heads(wk)\n",
        "    wq = self.split_heads(wq)\n",
        "    wv = self.split_heads(wv)\n",
        "\n",
        "    x, attn = self.scaled_dot_product_attention(wq, wk, wv, pad_mask)\n",
        "    x = self.out_proj(x) # (B, n_key, embed_dim)\n",
        "\n",
        "    return x, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  '''\n",
        "  RELU(SW(1)+b(1))W(2) + b(2)\n",
        "  W(1) & W(2) : (d, d)\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_dim, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, embedding_dim) #(d, d)\n",
        "    self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_ = self.linear1(x)\n",
        "    x_ = self.dropout(x_) # 이게 중간에 들어가는지 의문,,\n",
        "    x_ = self.relu(x_)\n",
        "    x_ = self.linear2(x_)\n",
        "    x_ = x + self.dropout(x_)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "zofos4MYYQHe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([[False, True], [True, False]]) # (2, 2)\n",
        "print(pad_mask.shape)\n",
        "similarity = torch.tensor([[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]) # (2, 2, 1, 2)\n",
        "print(similarity.shape)\n",
        "print(similarity)\n",
        "s =similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), 1e-9) # (B, 1, 1, n_key)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M73g9OWcd4R",
        "outputId": "8c711477-79a9-41c3-b84b-3ed61133b3a0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[1, 2]],\n",
            "\n",
            "         [[3, 4]]],\n",
            "\n",
            "\n",
            "        [[[5, 6]],\n",
            "\n",
            "         [[7, 8]]]])\n",
            "tensor([[[[1, 0]],\n",
            "\n",
            "         [[3, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 6]],\n",
            "\n",
            "         [[0, 8]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([False, True]) # (2)\n",
        "a = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2, 3)\n",
        "p = pad_mask.unsqueeze(-1)\n",
        "print(p)\n",
        "print(a.masked_fill(p, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lIBTTNe9ho",
        "outputId": "70702527-f969-4e9c-c3ae-fd7284d898f2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [ True]])\n",
            "tensor([[1, 2, 3],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "ju7YwMNFoQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_heads, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads)\n",
        "    self.feedforward = FeedForward(embedding_dim)\n",
        "    self.layernorm = nn.LayerNorm(embedding_dim, eps = 1e-8)\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    '''\n",
        "    Residual connection\n",
        "    Q, K, V로 모두 같은 값 사용\n",
        "\n",
        "    g(x) = x + Dropout(g(LayerNorm(x)))\n",
        "    '''\n",
        "    # Self attention Layer\n",
        "\n",
        "    x_ = self.layernorm(x)\n",
        "    res1, attn = self.attention.forward(x_, x_, x_, pad_mask = mask)\n",
        "    res1 = self.dropout(res1)\n",
        "\n",
        "    x = x + res1\n",
        "\n",
        "    # FFN\n",
        "    x_ = self.layernorm(x)\n",
        "    res2 = self.feedforward(x_)\n",
        "    res2 = self.dropout(res2)\n",
        "    x = x + res2\n",
        "\n",
        "    return x, attn"
      ],
      "metadata": {
        "id": "GEeLucgQoRiz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec model"
      ],
      "metadata": {
        "id": "jXfS54OEodFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "u_max =334730\n",
        "i_max =13047\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self, config, user_num = u_max, item_num = i_max):\n",
        "    super(SASRec, self).__init__()\n",
        "\n",
        "    self.user_num = user_num\n",
        "    self.item_num = item_num\n",
        "    self.config = config\n",
        "    self.num_blocks = config.num_blocks\n",
        "    self.embedding_dim = self.config.d\n",
        "    self.num_heads = config.num_heads\n",
        "\n",
        "    print(\"user_num:\", user_num) # maybe batch size\n",
        "    print(\"item_num:\", item_num)\n",
        "    print(\"embedding_dim\", self.embedding_dim)\n",
        "    print(\"num_heads:\", self.num_heads)\n",
        "    print(\"n:\", config.n)\n",
        "\n",
        "    # M : item embedding (N X d) 인데 item index는 1 부터 시작 => + 1해주기\n",
        "    self.M = torch.nn.Embedding(self.item_num + 1, self.config.d, padding_idx = 0) # 패딩은 0으로\n",
        "\n",
        "    self.pos_enc = nn.Parameter(torch.randn(self.config.n, self.config.d)) # learnable PE (n, d)\n",
        "\n",
        "    self.droptout = nn.Dropout(self.config.dropout)\n",
        "    self.encoders = nn.ModuleList()\n",
        "\n",
        "    for block in range(self.num_blocks):\n",
        "      self.encoders.append(EncoderBlock(self.embedding_dim, self.num_heads))\n",
        "\n",
        "    self.last_layernorm = torch.nn.LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "     x: (n_batch, seq_length = config.n - 2)\n",
        "     valid랑 test용 빠진 애들 들어오고\n",
        "    '''\n",
        "    print(\"x shape\", x.shape)\n",
        "    embedded_seq = self.M(torch.LongTensor(x)) # id는 정수들의 sequence (n_batch, config.n, embedding_dim)\n",
        "\n",
        "    embedded_seq = embedded_seq + self.pos_enc\n",
        "    pad_mask = torch.BoolTensor(x == 0).unsqueeze(-1)\n",
        "\n",
        "    # embedded_seq = embedded_seq.masked_fill(pad_mask.unsqueeze(-1))\n",
        "\n",
        "    # future item 가리기\n",
        "    attention_mask = ~torch.tril(torch.ones((x.shape[1], x.shape[1]), dtype = torch.bool))\n",
        "    print(\"attention\", attention_mask.shape, \"x\", x.shape)\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      embedded_seq = embedded_seq.masked_fill(pad_mask, 0)\n",
        "      embedded_seq, attn = encoder(embedded_seq, attention_mask)\n",
        "\n",
        "    output = embedded_seq # (B, n_key, embed_dim)\n",
        "    print(\"output\", output.shape)\n",
        "    # positive sequence : left shifted\n",
        "    seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    pos_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    neg_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "\n",
        "    for u in range(x.shape[0]):\n",
        "      pos_seq[2:] = x[u][:-2] # config.n\n",
        "\n",
        "    pos_embs = self.M(torch.LongTensor(pos_seq))\n",
        "    neg_embs = self.M(torch.LongTensor(neg_seq))\n"
      ],
      "metadata": {
        "id": "ugJIaNy61Sg6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 6\n",
        "num_heads = 3\n",
        "kdim = 3\n",
        "vdim = 3\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "my_attention = MultiheadAttention(\n",
        "    embed_dim = embed_dim,\n",
        "    num_heads = num_heads,\n",
        "    kdim = kdim,\n",
        "    vdim = vdim )\n",
        "\n",
        "print(my_attention)\n",
        "\n",
        "my_config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 5, # 300 for ml-1m\n",
        "    d = 3,\n",
        "    batch_size = 2,\n",
        "    test_batch_size = 1,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 20,\n",
        "    patience = 20,\n",
        "    num_heads =3\n",
        "\n",
        ")\n",
        "my_sasrec = SASRec(\n",
        "    my_config\n",
        ")\n",
        "us = [torch.tensor([1, 2, 3, 4, 5]), torch.tensor([6, 7, 8, 9, 10])]\n",
        "\n",
        "batch_us = torch.stack(us)\n",
        "\n",
        "my_sasrec.forward(batch_us)\n"
      ],
      "metadata": {
        "id": "v9kdiXmFJ-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003da189-47f2-4cc7-e36a-3ae20eb53bf5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiheadAttention(\n",
            "  (q_proj): Linear(in_features=6, out_features=6, bias=True)\n",
            "  (k_proj): Linear(in_features=3, out_features=6, bias=True)\n",
            "  (v_proj): Linear(in_features=3, out_features=6, bias=True)\n",
            "  (out_proj): Linear(in_features=6, out_features=6, bias=True)\n",
            ")\n",
            "user_num: 334730\n",
            "item_num: 13047\n",
            "embedding_dim 3\n",
            "num_heads: 3\n",
            "n: 5\n",
            "x shape torch.Size([2, 5])\n",
            "embedded_seq torch.Size([2, 5, 3])\n",
            "attention torch.Size([5, 5]) x torch.Size([2, 5])\n",
            "Encoder block input x torch.Size([2, 5, 3])\n",
            "Encoder block mask torch.Size([5, 5])\n",
            "Multihead input q torch.Size([2, 5, 3])\n",
            "wq (B, num_heads, n_seq, embed_dim) torch.Size([2, 3, 5, 1])\n",
            "torch.Size([5, 5]) pad_mask\n",
            "After unsqueeze torch.Size([1, 1, 5, 5])\n",
            "Similairity torch.Size([2, 3, 5, 5])\n",
            "similarity torch.Size([2, 3, 5, 5])\n",
            "attention_value torch.Size([2, 5, 3])\n",
            "x, attn torch.Size([2, 5, 3]) torch.Size([2, 5, 5])\n",
            "res1 torch.Size([2, 5, 3])\n",
            "Encoder block input x torch.Size([2, 5, 3])\n",
            "Encoder block mask torch.Size([5, 5])\n",
            "Multihead input q torch.Size([2, 5, 3])\n",
            "wq (B, num_heads, n_seq, embed_dim) torch.Size([2, 3, 5, 1])\n",
            "torch.Size([5, 5]) pad_mask\n",
            "After unsqueeze torch.Size([1, 1, 5, 5])\n",
            "Similairity torch.Size([2, 3, 5, 5])\n",
            "similarity torch.Size([2, 3, 5, 5])\n",
            "attention_value torch.Size([2, 5, 3])\n",
            "x, attn torch.Size([2, 5, 3]) torch.Size([2, 5, 5])\n",
            "res1 torch.Size([2, 5, 3])\n",
            "output torch.Size([2, 5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "kyHk6zAJfMM9",
        "outputId": "6300e551-394d-469c-ae22-343126f1c830"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-640da0ca6e42>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_sasrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-aac60c7a7cf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# future item 가리기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = ~torch.tril(torch.ones((5, 5), dtype = torch.bool))\n",
        "print(attention_mask)\n",
        "print(attention_mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HgLdtqzfqFc",
        "outputId": "8003a894-e012-4842-d1e3-dab128233c49"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True],\n",
            "        [False, False, False,  True,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False, False]])\n",
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-pElBdff8Eq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}