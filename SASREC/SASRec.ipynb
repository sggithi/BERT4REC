{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "etC2-FZjeyHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a293f6-1868-408d-d090-4620d106bf16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/RecommanderSystems')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/RecommanderSystems')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 50, # 300 for ml-1m\n",
        "    d = 40,\n",
        "    batch_size = 128,\n",
        "    test_batch_size = 100,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 200,\n",
        "    patience = 20\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "--u0DvNahuJl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "SASRec의 경우 학습을 시킬 때 user 당 item의 시퀀스가 필요"
      ],
      "metadata": {
        "id": "mb6FLTlPtzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_item_sequence_dict = defaultdict(list)\n",
        "\n",
        "u_max = -1\n",
        "i_max = -1\n",
        "\n",
        "data = open(config.data, 'r')\n",
        "for line in data:\n",
        "  u, i = line.strip().split(' ') # (user, item) 조합의 txt 파일 읽기\n",
        "  u = int(u)\n",
        "  i = int(i)\n",
        "  user_item_sequence_dict[u].append(i)\n",
        "  if u > u_max:\n",
        "    u_max = u\n",
        "  if i > i_max:\n",
        "    i_max = i\n",
        "\n",
        "user_seq = {}\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "for u in user_item_sequence_dict:\n",
        "  seq = user_item_sequence_dict[u]\n",
        "  l = len(seq)\n",
        "  user_valid[u] = []\n",
        "  user_test[u] = []\n",
        "  if l < 3:\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "  elif l < config.n:\n",
        "    # padding 필요\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2] # n - 1번째\n",
        "    user_test[u] = padded_seq[-1] # n번째\n",
        "  else:\n",
        "    # l > config.n\n",
        "    padded_seq = seq[-config.n:]\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2]\n",
        "    user_test[u] = padded_seq[-1]\n",
        "  user_seq[u] = padded_seq\n",
        "\n",
        "print(\"u_max\", u_max)\n",
        "print(\"i_max\", i_max)\n",
        "print(len(padded_seq))"
      ],
      "metadata": {
        "id": "uxiK7K70izuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3cc1f03-8b97-4ee4-fc17-66daf5e594a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_max 334730\n",
            "i_max 13047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head"
      ],
      "metadata": {
        "id": "RO8eEmvVYQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, kdim = None, vdim = None):\n",
        "    \"\"\"\n",
        "\n",
        "    embed_dim: Dimensionality of the input and output embeddings.\n",
        "    num_heads: Number of attention heads.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.kdim = kdim if kdim is not None else embed_dim\n",
        "    self.vdim = vdim if vdim is not None else embed_dim\n",
        "\n",
        "    # 그냥 Q, K, V 사이즈 통일\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.kdim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.vdim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    # nn.init.uniform_(self.q_proj.bias)\n",
        "    # nn.init.uniform_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.q_proj.bias)\n",
        "    nn.init.zeros_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.v_proj.bias)\n",
        "    nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Head 늘리는 방식 중에 embedding 사이즈 줄여서 head수 늘리는 방식\n",
        "\n",
        "    x : (batch_size, -1, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "      (batch_size, num_heads, -1, embed_dim // num_heads).\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = x.shape[0]\n",
        "    splited = x.reshape(n_batch, -1, self.num_heads, self.embed_dim // self.num_heads)\n",
        "\n",
        "    return splited.transpose(1, 2) # (B, num_heads, -1, embed_dim // num_heads)\n",
        "\n",
        "  def scaled_dot_product_attention(self, wq, wk, wv, pad_mask = None):\n",
        "    \"\"\"\n",
        "\n",
        "    wq, wk, wv: (B, num_heads, n_seq, embed_dim // num_heads).\n",
        "\n",
        "    Returns:\n",
        "      Scaled Dot-Product Attention  (B, n_seq, embed_dim).\n",
        "      Average attention weights across heads (B, n_seq, n_key) -시각화에 사용\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_batch = wq.shape[0]\n",
        "    d_k = self.embed_dim // self.num_heads\n",
        "\n",
        "    \"\"\"\n",
        "    wq @ wk.T: (B, num_heads, n_seq, n_key)\n",
        "    pad_mask: (B, n_key)\n",
        "    softmax @ v: (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    \"\"\"\n",
        "\n",
        "    wk_t = wk.transpose(2, 3) # (B, num_heads, embed_dim // num_heads, n_key)\n",
        "\n",
        "    similarity = torch.matmul(wq, wk_t) # (B, num_heads, n_seq, n_key)\n",
        "    similarity /= math.sqrt(d_k)\n",
        "    epsilon = -1e9\n",
        "\n",
        "    if pad_mask is not None:\n",
        "      similarity = similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), epsilon) # (B, 1, 1, n_key)\n",
        "\n",
        "    softmax_sim = torch.softmax(similarity, dim = 3) # key마다 얼마나 비슷 -> softmax (B, num_heads, n_seq, n_key)\n",
        "    average_att_w = torch.mean(softmax_sim, dim = 1) # head 마다 (B, n_seq, n_key) 평균\n",
        "\n",
        "    # average_att_w 찍어보면 각 key 마다의 활성화정도 시각화 가능\n",
        "\n",
        "    attention_value = torch.matmul(softmax_sim, wv) # (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    attention_value = attention_value.transpose(1, 2) # (B, n_key, num_heads, embed_dim // num_heads)\n",
        "    # 사이즈 원상 복구 근데 이게 맞나??? 좀 이상함,,\n",
        "    attention_value = attention_value.reshape(n_batch, -1, self.embed_dim) # (B, n_key, embed_dim)\n",
        "\n",
        "    return attention_value, average_att_w\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    \"\"\"\n",
        "    q: (B, n_seq, embed_dim)\n",
        "    k: (B, n_key, kdim)\n",
        "    v: (B, n_key, vdim)\n",
        "    pad_mask: (B, n_key)\n",
        "\n",
        "    Returns:\n",
        "      output (B, -1, embed_dim)\n",
        "      average_att_w: (B, n_seq, n_key)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    wq = self.q_proj(q) #(B, n_seq, embed_dim)\n",
        "    wk = self.k_proj(k) #(B, n_key, kdim)\n",
        "    wv = self.v_proj(v) # #(B, n_key, embed_dim)\n",
        "\n",
        "    wk = self.split_heads(wk)\n",
        "    wq = self.split_heads(wq)\n",
        "    wv = self.split_heads(wv)\n",
        "\n",
        "    x, attn = self.scaled_dot_product_attention(wq, wk, wv, pad_mask)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  '''\n",
        "  RELU(SW(1)+b(1))W(2) + b(2)\n",
        "  W(1) & W(2) : (d, d)\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_dim, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, embedding_dim) #(d, d)\n",
        "    self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_ = self.linear1(x_)\n",
        "    x_ = self.dropout(x_) # 이게 중간에 들어가는지 의문,,\n",
        "    x_ = self.relu(x_)\n",
        "    x_ = self.linear2(x_)\n",
        "    x_ = x + self.dropout(x_)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "zofos4MYYQHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([[False, True], [True, False]])\n",
        "print(pad_mask.shape)\n",
        "similarity = torch.tensor([[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]) # (2, 2, 1, 2)\n",
        "print(similarity.shape)\n",
        "print(similarity)\n",
        "s =similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), 1e-9) # (B, 1, 1, n_key)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M73g9OWcd4R",
        "outputId": "e6c1457d-f4da-447a-e969-67a1cb7747b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[1, 2]],\n",
            "\n",
            "         [[3, 4]]],\n",
            "\n",
            "\n",
            "        [[[5, 6]],\n",
            "\n",
            "         [[7, 8]]]])\n",
            "tensor([[[[1, 0]],\n",
            "\n",
            "         [[3, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 6]],\n",
            "\n",
            "         [[0, 8]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "ju7YwMNFoQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_heads, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads)\n",
        "    self.feedforward = FeedForward(embedding_dim)\n",
        "    self.layernorm = nn.LayerNorm(embedding_dim, eps = 1e-8)\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    '''\n",
        "    Residual connection\n",
        "    Q, K, V로 모두 같은 값 사용\n",
        "\n",
        "    g(x) = x + Dropout(g(LayerNorm(x)))\n",
        "    '''\n",
        "    # Self attention Layer\n",
        "    x_ = self.layernorm(x)\n",
        "    res1, attn = self.attention.forward(x_, x_, x_, pad_mask = mask)\n",
        "    res1 = self.dropout(res1)\n",
        "    x = x + res1\n",
        "    # FFN\n",
        "    x_ = self.layernorm(x)\n",
        "    res2 = self.feedforward(x_)\n",
        "    res2 = self.dropout(res2)\n",
        "    x = x + res2\n",
        "\n",
        "    return x, attn"
      ],
      "metadata": {
        "id": "GEeLucgQoRiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec model"
      ],
      "metadata": {
        "id": "jXfS54OEodFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self, config, user_num = u_max, item_num = i_max):\n",
        "    super(SASRec, self).__init__()\n",
        "\n",
        "    self.user_num = user_num\n",
        "    self.item_num = item_num\n",
        "    self.config = config\n",
        "    self.num_blocks = config.num_blocks\n",
        "    self.embedding_dim = self.config.d\n",
        "    self.num_heads = config.num_heads\n",
        "\n",
        "    print(\"user_num:\", user_num) # maybe batch size\n",
        "    print(\"item_num:\", item_num)\n",
        "    print(\"embedding_dim\", self.embedding_dim)\n",
        "    print(\"num_heads:\", self.num_heads)\n",
        "    print(\"n:\", config.n)\n",
        "\n",
        "    # M : item embedding (N X d) 인데 item index는 1 부터 시작 => + 1해주기\n",
        "    self.M = torch.nn.Embedding(self.item_num + 1, self.config.d, padding_idx = 0) # 패딩은 0으로\n",
        "\n",
        "    self.pos_enc = nn.Parameter(torch.randn(self.config.n, self.config.d)) # learnable PE (n, d)\n",
        "    # nn.Embedding(self.config.n, self.config.d)\n",
        "    self.droptout = nn.Dropout(self.config.dropout)\n",
        "    self.encoders = nn.ModuleList()\n",
        "\n",
        "    for block in self.num_blocks:\n",
        "      self.encoders.append(EncoderBlock(self.embedding_dim, self.num_head))\n",
        "\n",
        "    self.last_layernorm = torch.nn.LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "     x: (n_batch, seq_length = config.n - 2)\n",
        "     valid랑 test용 빠진 애들 들어오고\n",
        "    '''\n",
        "    embedded_seq = self.M(torch.LongTensor(x)) # id는 정수들의 sequence (n_batch, config.n, embedding_dim)\n",
        "    embedded_seq = embedded_seq + self.pos_enc\n",
        "    pad_mask = torch.BoolTensor(x == 0)\n",
        "    # embedded_seq = embedded_seq.masked_fill(pad_mask.unsqueeze(-1))\n",
        "\n",
        "    # future item 가리기\n",
        "    attention_mask = ~torch.tril(torch.ones(x.shape[1], x.shape[1]), dtype = torch.bool)\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      embedded_seq = embedded_seq.masked_fill(pad_mask.unsqueeze(-1))\n",
        "      embedded_seq, attn = encoder(embedded_seq, attention_mask)\n",
        "\n",
        "    output = embedded_seq\n",
        "    # positive sequence : left shifted\n",
        "    seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    pos_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    neg_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "\n",
        "    for u in range(x.shape[0]):\n",
        "      pos_seq[2:] = x[u][:-2] # config.n\n",
        "\n",
        "    pos_embs = self.M(torch.LongTensor(pos_seq))\n",
        "    neg_embs = self.M(torch.LongTensor(neg_seq))\n"
      ],
      "metadata": {
        "id": "ugJIaNy61Sg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [0, 1]\n",
        "print(a[0:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHgTf3_l4dxE",
        "outputId": "94a41bbc-07fc-432f-93b9-8db24f4213cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9kdiXmFJ-Uj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}