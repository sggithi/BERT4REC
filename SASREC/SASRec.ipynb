{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "etC2-FZjeyHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad6d4a9-6e09-4469-f3a8-1b1d267282ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/RecommanderSystems')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/RecommanderSystems')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 50, # 300 for ml-1m, 50 for others\n",
        "    d = 40,\n",
        "    batch_size = 128,\n",
        "    num_heads = 1, # default\n",
        "    test_batch_size = 100,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 200,\n",
        "    patience = 20\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "--u0DvNahuJl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "SASRec의 경우 학습을 시킬 때 user 당 item의 시퀀스가 필요"
      ],
      "metadata": {
        "id": "mb6FLTlPtzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_item_sequence_dict = defaultdict(list)\n",
        "\n",
        "u_max = -1\n",
        "i_max = -1\n",
        "\n",
        "data = open(config.data, 'r')\n",
        "for line in data:\n",
        "  u, i = line.strip().split(' ') # (user, item) 조합의 txt 파일 읽기\n",
        "  u = int(u)\n",
        "  i = int(i)\n",
        "  user_item_sequence_dict[u].append(i)\n",
        "  if u > u_max:\n",
        "    u_max = u\n",
        "  if i > i_max:\n",
        "    i_max = i\n",
        "\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "for u in user_item_sequence_dict:\n",
        "  seq = user_item_sequence_dict[u]\n",
        "  l = len(seq)\n",
        "  user_valid[u] = []\n",
        "  user_test[u] = []\n",
        "  if l < 3:\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "  elif l >= 52:\n",
        "    user_train[u] = seq[-config.n - 2:-2]\n",
        "    user_valid[u] = seq[-2]\n",
        "    user_test[u] = seq[-1]\n",
        "\n",
        "  else:\n",
        "    user_valid[u] = seq[-2]\n",
        "    user_test[u] = seq[-1]\n",
        "    padded_seq = [0] * (config.n - l + 2) + seq[:-2]\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "print(\"u_max\", u_max)\n",
        "print(\"i_max\", i_max)\n",
        "print(len(user_train[1]))"
      ],
      "metadata": {
        "id": "uxiK7K70izuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b2d009-b95e-4c4e-c01e-c56d574fd7e1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_max 334730\n",
            "i_max 13047\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head"
      ],
      "metadata": {
        "id": "RO8eEmvVYQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, kdim = None, vdim = None):\n",
        "    \"\"\"\n",
        "\n",
        "    embed_dim: Dimensionality of the input and output embeddings.\n",
        "    num_heads: Number of attention heads.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.kdim = kdim if kdim is not None else embed_dim\n",
        "    self.vdim = vdim if vdim is not None else embed_dim\n",
        "\n",
        "    # 그냥 Q, K, V 사이즈 통일\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.kdim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.vdim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    # nn.init.uniform_(self.q_proj.bias)\n",
        "    # nn.init.uniform_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.q_proj.bias)\n",
        "    nn.init.zeros_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.v_proj.bias)\n",
        "    nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Head 늘리는 방식 중에 embedding 사이즈 줄여서 head수 늘리는 방식\n",
        "\n",
        "    x : (batch_size, -1, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "      (batch_size, num_heads, -1, embed_dim // num_heads).\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = x.shape[0]\n",
        "    splited = x.reshape(n_batch, -1, self.num_heads, self.embed_dim // self.num_heads)\n",
        "\n",
        "    return splited.transpose(1, 2) # (B, num_heads, -1, embed_dim // num_heads)\n",
        "\n",
        "  def scaled_dot_product_attention(self, wq, wk, wv, pad_mask = None):\n",
        "    \"\"\"\n",
        "\n",
        "    wq, wk, wv: (B, num_heads, n_seq, embed_dim // num_heads).\n",
        "\n",
        "    Returns:\n",
        "      Scaled Dot-Product Attention  (B, n_seq, embed_dim).\n",
        "      Average attention weights across heads (B, n_seq, n_key) -시각화에 사용\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_batch = wq.shape[0]\n",
        "    d_k = self.embed_dim // self.num_heads\n",
        "\n",
        "    \"\"\"\n",
        "    wq @ wk.T: (B, num_heads, n_seq, n_key)\n",
        "    pad_mask: (B, n_key)\n",
        "    softmax @ v: (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    \"\"\"\n",
        "\n",
        "    wk_t = wk.transpose(2, 3) # (B, num_heads, embed_dim // num_heads, n_key)\n",
        "\n",
        "    similarity = torch.matmul(wq, wk_t) # (B, num_heads, n_seq, n_key)\n",
        "    similarity /= math.sqrt(d_k)\n",
        "    epsilon = -1e9\n",
        "\n",
        "    # pad_mask가 여기서는 tril로 들어와서 (n, n)으로 들어옴 => (1, 1, n, n)으로 unsqueeze\n",
        "    if pad_mask is not None:\n",
        "      similarity = similarity.masked_fill(pad_mask.unsqueeze(0).unsqueeze(1), epsilon) # (B, 1, 1, n_key)\n",
        "\n",
        "    softmax_sim = torch.softmax(similarity, dim = 3) # key마다 얼마나 비슷 -> softmax (B, num_heads, n_seq, n_key)\n",
        "    average_att_w = torch.mean(softmax_sim, dim = 1) # head 마다 (B, n_seq, n_key) 평균\n",
        "\n",
        "    # average_att_w 찍어보면 각 key 마다의 활성화정도 시각화 가능\n",
        "\n",
        "    attention_value = torch.matmul(softmax_sim, wv) # (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    attention_value = attention_value.transpose(1, 2) # (B, n_key, num_heads, embed_dim // num_heads)\n",
        "    # 사이즈 원상 복구 근데 이게 맞나??? 좀 이상함,,\n",
        "    attention_value = attention_value.reshape(n_batch, -1, self.embed_dim) # (B, n_key, embed_dim)\n",
        "\n",
        "    return attention_value, average_att_w\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    \"\"\"\n",
        "    q: (B, n_seq, embed_dim)\n",
        "    k: (B, n_key, kdim)\n",
        "    v: (B, n_key, vdim)\n",
        "    pad_mask: (B, n_key)\n",
        "\n",
        "    Returns:\n",
        "      output (B, -1, embed_dim)\n",
        "      average_att_w: (B, n_seq, n_key)\n",
        "\n",
        "    \"\"\"\n",
        "    # print(\"Multihead input q\", q.shape)\n",
        "    wq = self.q_proj(q) #(B, n_seq, embed_dim)\n",
        "    wk = self.k_proj(k) #(B, n_key, kdim)\n",
        "    wv = self.v_proj(v) # #(B, n_key, embed_dim)\n",
        "\n",
        "    wk = self.split_heads(wk)\n",
        "    wq = self.split_heads(wq)\n",
        "    wv = self.split_heads(wv)\n",
        "\n",
        "    x, attn = self.scaled_dot_product_attention(wq, wk, wv, pad_mask)\n",
        "    x = self.out_proj(x) # (B, n_key, embed_dim)\n",
        "\n",
        "    return x, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  '''\n",
        "  RELU(SW(1)+b(1))W(2) + b(2)\n",
        "  W(1) & W(2) : (d, d)\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_dim, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, embedding_dim) #(d, d)\n",
        "    self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_ = self.linear1(x)\n",
        "    x_ = self.dropout(x_) # 이게 중간에 들어가는지 의문,,\n",
        "    x_ = self.relu(x_)\n",
        "    x_ = self.linear2(x_)\n",
        "    x_ = x + self.dropout(x_)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "zofos4MYYQHe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([[False, True], [True, False]]) # (2, 2)\n",
        "print(pad_mask.shape)\n",
        "similarity = torch.tensor([[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]) # (2, 2, 1, 2)\n",
        "print(similarity.shape)\n",
        "print(similarity)\n",
        "s =similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), 1e-9) # (B, 1, 1, n_key)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M73g9OWcd4R",
        "outputId": "4294ff58-b48c-418b-8376-a0160fe2a6d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[1, 2]],\n",
            "\n",
            "         [[3, 4]]],\n",
            "\n",
            "\n",
            "        [[[5, 6]],\n",
            "\n",
            "         [[7, 8]]]])\n",
            "tensor([[[[1, 0]],\n",
            "\n",
            "         [[3, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 6]],\n",
            "\n",
            "         [[0, 8]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([False, True]) # (2)\n",
        "a = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2, 3)\n",
        "p = pad_mask.unsqueeze(-1)\n",
        "print(p)\n",
        "print(a.masked_fill(p, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lIBTTNe9ho",
        "outputId": "70702527-f969-4e9c-c3ae-fd7284d898f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [ True]])\n",
            "tensor([[1, 2, 3],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "ju7YwMNFoQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_heads, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads)\n",
        "    self.feedforward = FeedForward(embedding_dim)\n",
        "    self.layernorm = nn.LayerNorm(embedding_dim, eps = 1e-8)\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    '''\n",
        "    Residual connection\n",
        "    Q, K, V로 모두 같은 값 사용\n",
        "\n",
        "    g(x) = x + Dropout(g(LayerNorm(x)))\n",
        "    '''\n",
        "    # Self attention Layer\n",
        "\n",
        "    x_ = self.layernorm(x)\n",
        "    res1, attn = self.attention.forward(x_, x_, x_, pad_mask = mask)\n",
        "    res1 = self.dropout(res1)\n",
        "\n",
        "    x = x + res1\n",
        "\n",
        "    # FFN\n",
        "    x_ = self.layernorm(x)\n",
        "    res2 = self.feedforward(x_)\n",
        "    res2 = self.dropout(res2)\n",
        "    x = x + res2\n",
        "\n",
        "    return x, attn"
      ],
      "metadata": {
        "id": "GEeLucgQoRiz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "xw4ZcLi9RCpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class UserTrainDataset(Dataset):\n",
        "  def __init__(self, user_train):\n",
        "    self.user_train = user_train\n",
        "\n",
        "    self.users = list(user_train.keys())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    user_id = self.users[idx]\n",
        "    user_data = self.user_train[user_id]\n",
        "\n",
        "    return user_id, user_data\n",
        "\n",
        "def collate_fn(batch):\n",
        "  user_ids, user_data = zip(*batch)\n",
        "  user_data_tensor = torch.tensor(user_data, dtype=torch.long)  # Assuming user_data is a list of sequences\n",
        "  return user_ids, user_data_tensor\n",
        "\n",
        "user_train_dataset = UserTrainDataset(user_train)\n",
        "user_train_dataloader = DataLoader(user_train_dataset, batch_size= config.batch_size, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "qdf6c3ynRBnr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec model"
      ],
      "metadata": {
        "id": "jXfS54OEodFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# user_train_dataset = UserTrainDataset(user_train)\n",
        "# u_max =5\n",
        "# i_max =10\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self, config, user_train_dataloader = user_train_dataloader, user_train = user_train, user_num = u_max, item_num = i_max):\n",
        "    super(SASRec, self).__init__()\n",
        "\n",
        "    self.user_num = user_num\n",
        "    self.item_num = item_num\n",
        "    self.config = config\n",
        "    self.num_blocks = config.num_blocks\n",
        "    self.embedding_dim = self.config.d\n",
        "    self.num_heads = config.num_heads\n",
        "    self.user_seq_dataloader = user_train_dataloader\n",
        "    self.user_train = user_train\n",
        "\n",
        "    print(\"user_num:\", user_num) # maybe batch size\n",
        "    print(\"item_num:\", item_num)\n",
        "    print(\"embedding_dim\", self.embedding_dim)\n",
        "    print(\"num_heads:\", self.num_heads)\n",
        "    print(\"n:\", config.n)\n",
        "\n",
        "    # M : item embedding (N X d) 인데 item index는 1 부터 시작 => + 1해주기\n",
        "    self.M = torch.nn.Embedding(self.item_num + 1, self.config.d, padding_idx = 0) # 패딩은 0으로\n",
        "\n",
        "    self.pos_enc = nn.Parameter(torch.randn(self.config.n, self.config.d)) # learnable PE (n, d)\n",
        "\n",
        "    self.droptout = nn.Dropout(self.config.dropout)\n",
        "    self.encoders = nn.ModuleList()\n",
        "\n",
        "    for block in range(self.num_blocks):\n",
        "      self.encoders.append(EncoderBlock(self.embedding_dim, self.num_heads))\n",
        "\n",
        "    self.last_layernorm = torch.nn.LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "    self.criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(), lr=config.lr)\n",
        "\n",
        "  def forward(self, user_ids, x):\n",
        "    '''\n",
        "    x: (n_batch, config.n)\n",
        "    user_train[idx]\n",
        "    '''\n",
        "    # print(\"x\", x.shape)\n",
        "    embedded_seq = self.M(torch.LongTensor(x)) # id는 정수들의 sequence (n_batch, config.n, embedding_dim)\n",
        "    embedded_seq = embedded_seq + self.pos_enc\n",
        "    pad_mask = torch.BoolTensor(x == 0).unsqueeze(-1)\n",
        "    # future item 가리기\n",
        "    attention_mask = ~torch.tril(torch.ones((x.shape[1], x.shape[1]), dtype = torch.bool))\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      embedded_seq = embedded_seq.masked_fill(pad_mask, 0)\n",
        "      embedded_seq, attn = encoder(embedded_seq, attention_mask)\n",
        "\n",
        "    output = self.last_layernorm(embedded_seq) # (B, n_key, embed_dim)\n",
        "    # print(\"output\", output.shape)\n",
        "    l = 0\n",
        "    rot_list = []\n",
        "    rjt_list = []\n",
        "    seq_l = []\n",
        "    pos = []\n",
        "    neg = []\n",
        "\n",
        "    for l, u in enumerate(user_ids):\n",
        "      seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "      pos_seq = np.zeros([self.config.n], dtype = np.int32) # positive sequence : left shifted\n",
        "      neg_seq = np.zeros([self.config.n], dtype = np.int32) # Random negative\n",
        "      answer = x[l][-1]\n",
        "      idx = self.config.n - 1\n",
        "\n",
        "      for item in reversed(x[l]):\n",
        "        seq[idx] = item\n",
        "        pos_seq[idx] = answer\n",
        "        if answer != 0:\n",
        "          ns = np.random.randint(1, self.item_num + 1)\n",
        "          while ns in self.user_train[u]:\n",
        "            ns = np.random.randint(1, self.item_num + 1)\n",
        "          neg_seq[idx] = ns\n",
        "          answer = item\n",
        "          if idx == 0:\n",
        "            break\n",
        "          idx -= 1\n",
        "\n",
        "      seq_l.append(torch.LongTensor(seq))\n",
        "      pos.append(torch.LongTensor(pos_seq))\n",
        "      neg.append(torch.LongTensor(neg_seq))\n",
        "\n",
        "\n",
        "      # pos_embs = self.M(torch.LongTensor(pos_seq))\n",
        "      # neg_embs = self.M(torch.LongTensor(neg_seq))\n",
        "\n",
        "      # print(\"output\", output.shape, \"pos_embs\", pos_embs.shape)\n",
        "      # # similarity\n",
        "      # rot = (output * pos_embs).sum(dim = - 1) # (n_batch, n, d) => (n_batch, n)\n",
        "      # rjt = (output * neg_embs).sum(dim = - 1) # (n_batch, n, d) => (n_batch, n)\n",
        "\n",
        "      # indices = np.where(seq != 0)\n",
        "      #rot_list.append(rot)\n",
        "      #rjt_list.append(rjt)\n",
        "\n",
        "    seq_l = torch.stack(seq_l)\n",
        "    pos_embs = self.M(torch.LongTensor(torch.stack(pos)))\n",
        "    neg_embs = self.M(torch.LongTensor(torch.stack(neg)))\n",
        "    rot = (output * pos_embs).sum(dim = - 1)\n",
        "    rjt = (output * neg_embs).sum(dim = - 1)\n",
        "\n",
        "\n",
        "    #rot_tensor = torch.stack(rot_list)\n",
        "    #rjt_tensor = torch.stack(rjt_list)\n",
        "    indicies = np.where( seq_l != 0)\n",
        "    #print(\"rot_tensor\", rot.shape)\n",
        "    return rot, rjt, indicies\n",
        "\n",
        "  def train(self):\n",
        "    '''\n",
        "     batch: (n_batch, seq_length = config.n - 2)\n",
        "     user_seq -2 / -1 / :-2 valid / test / train\n",
        "     '''\n",
        "    for epoch in range(self.config.epoch):\n",
        "      batch_num = 0\n",
        "      for idx, batch in self.user_seq_dataloader:\n",
        "        rot, rjt, indices = self.forward(idx, batch)\n",
        "        pos_labels, neg_labels = torch.ones(rot.shape), torch.zeros(rjt.shape)\n",
        "        self.optimizer.zero_grad()\n",
        "        try:\n",
        "          loss = self.criterion(rot[indices], pos_labels[indices]) + self.criterion(rjt[indices], neg_labels[indices])\n",
        "        except:\n",
        "          print(\"indice\", indices, \"pos_labels\", pos_labels.shape, neg_labels.shape)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if batch_num % 500 == 0:\n",
        "          print(\"Epoch\", epoch, \"Batch_num\", batch_num, \"loss\", loss.item())\n",
        "        batch_num += 1\n",
        "      print(\"loss in epoch {}: {}\".format(epoch,loss.item()))\n",
        "      torch.save(model.state_dict(), 'sasrec_model.pth')\n",
        "      #if epoch % 20 == 0:\n",
        "\n",
        "\n",
        "        #print(\"Evalution in epoch {} iteration {}: {}\".format(epoch, loss.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ugJIaNy61Sg6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SASRec(config)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5HgLdtqzfqFc",
        "outputId": "399ac7ce-1ebd-4bca-b23b-24595df1b25c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_num: 334730\n",
            "item_num: 13047\n",
            "embedding_dim 40\n",
            "num_heads: 1\n",
            "n: 50\n",
            "Epoch 0 Batch_num 0 loss 4.843395233154297\n",
            "Epoch 0 Batch_num 50 loss 4.69819974899292\n",
            "Epoch 0 Batch_num 100 loss 4.323727607727051\n",
            "Epoch 0 Batch_num 150 loss 3.695662498474121\n",
            "Epoch 0 Batch_num 200 loss 3.38462495803833\n",
            "Epoch 0 Batch_num 250 loss 2.6329727172851562\n",
            "Epoch 0 Batch_num 300 loss 2.4251203536987305\n",
            "Epoch 0 Batch_num 350 loss 2.179605722427368\n",
            "Epoch 0 Batch_num 400 loss 2.010875701904297\n",
            "Epoch 0 Batch_num 450 loss 1.7281379699707031\n",
            "Epoch 0 Batch_num 500 loss 1.7461698055267334\n",
            "Epoch 0 Batch_num 550 loss 1.6226662397384644\n",
            "Epoch 0 Batch_num 600 loss 1.6184031963348389\n",
            "Epoch 0 Batch_num 650 loss 1.553235650062561\n",
            "Epoch 0 Batch_num 700 loss 1.500486135482788\n",
            "Epoch 0 Batch_num 750 loss 1.4583523273468018\n",
            "Epoch 0 Batch_num 800 loss 1.4331953525543213\n",
            "Epoch 0 Batch_num 850 loss 1.372255802154541\n",
            "Epoch 0 Batch_num 900 loss 1.3444125652313232\n",
            "Epoch 0 Batch_num 950 loss 1.3614170551300049\n",
            "Epoch 0 Batch_num 1000 loss 1.3345105648040771\n",
            "Epoch 0 Batch_num 1050 loss 1.3424155712127686\n",
            "Epoch 0 Batch_num 1100 loss 1.2623220682144165\n",
            "Epoch 0 Batch_num 1150 loss 1.3250752687454224\n",
            "Epoch 0 Batch_num 1200 loss 1.2694981098175049\n",
            "Epoch 0 Batch_num 1250 loss 1.2381651401519775\n",
            "Epoch 0 Batch_num 1300 loss 1.2293113470077515\n",
            "Epoch 0 Batch_num 1350 loss 1.2233293056488037\n",
            "Epoch 0 Batch_num 1400 loss 1.1708344221115112\n",
            "Epoch 0 Batch_num 1450 loss 1.1805202960968018\n",
            "Epoch 0 Batch_num 1500 loss 1.1501821279525757\n",
            "Epoch 0 Batch_num 1550 loss 1.1487189531326294\n",
            "Epoch 0 Batch_num 1600 loss 1.1020292043685913\n",
            "Epoch 0 Batch_num 1650 loss 1.1191726922988892\n",
            "Epoch 0 Batch_num 1700 loss 1.1022498607635498\n",
            "Epoch 0 Batch_num 1750 loss 1.0233155488967896\n",
            "Epoch 0 Batch_num 1800 loss 1.0453070402145386\n",
            "Epoch 0 Batch_num 1850 loss 0.986286997795105\n",
            "Epoch 0 Batch_num 1900 loss 0.9696463346481323\n",
            "Epoch 0 Batch_num 1950 loss 0.9662160873413086\n",
            "Epoch 0 Batch_num 2000 loss 0.919813871383667\n",
            "Epoch 0 Batch_num 2050 loss 0.8887594938278198\n",
            "Epoch 0 Batch_num 2100 loss 0.9377846717834473\n",
            "Epoch 0 Batch_num 2150 loss 0.9170210361480713\n",
            "Epoch 0 Batch_num 2200 loss 0.8850786685943604\n",
            "Epoch 0 Batch_num 2250 loss 0.8961626887321472\n",
            "Epoch 0 Batch_num 2300 loss 0.8925065994262695\n",
            "Epoch 0 Batch_num 2350 loss 0.862310528755188\n",
            "Epoch 0 Batch_num 2400 loss 0.8089672327041626\n",
            "Epoch 0 Batch_num 2450 loss 0.8577278852462769\n",
            "Epoch 0 Batch_num 2500 loss 0.801089882850647\n",
            "Epoch 0 Batch_num 2550 loss 0.8070939779281616\n",
            "Epoch 0 Batch_num 2600 loss 0.7423655986785889\n",
            "loss in epoch 0: 0.968055009841919\n",
            "Epoch 1 Batch_num 0 loss 0.7394479513168335\n",
            "Epoch 1 Batch_num 50 loss 0.7328177094459534\n",
            "Epoch 1 Batch_num 100 loss 0.773444652557373\n",
            "Epoch 1 Batch_num 150 loss 0.7208852767944336\n",
            "Epoch 1 Batch_num 200 loss 0.7617735862731934\n",
            "Epoch 1 Batch_num 250 loss 0.7464489936828613\n",
            "Epoch 1 Batch_num 300 loss 0.7516618967056274\n",
            "Epoch 1 Batch_num 350 loss 0.7711644172668457\n",
            "Epoch 1 Batch_num 400 loss 0.7024672031402588\n",
            "Epoch 1 Batch_num 450 loss 0.7914466857910156\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-f6bb7ff43319>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSASRec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-97-9fbd1adad269>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pos_labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = SASRec(config)\n",
        "loaded_model.load_state_dict(torch.load('sasrec_model.pth'))\n",
        "loaded_model.train()"
      ],
      "metadata": {
        "id": "jydjLiUUNRyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-pElBdff8Eq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}