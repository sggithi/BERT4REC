{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "etC2-FZjeyHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf462de-be1a-4b9f-f6a8-8d617bd57dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/RecommanderSystems')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/RecommanderSystems')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 50, # 300 for ml-1m, 50 for others\n",
        "    d = 40,\n",
        "    batch_size = 128,\n",
        "    num_heads = 1, # default\n",
        "    test_batch_size = 100,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 200,\n",
        "    patience = 20\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "--u0DvNahuJl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "SASRec의 경우 학습을 시킬 때 user 당 item의 시퀀스가 필요"
      ],
      "metadata": {
        "id": "mb6FLTlPtzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_item_sequence_dict = defaultdict(list)\n",
        "\n",
        "u_max = -1\n",
        "i_max = -1\n",
        "\n",
        "data = open(config.data, 'r')\n",
        "for line in data:\n",
        "  u, i = line.strip().split(' ') # (user, item) 조합의 txt 파일 읽기\n",
        "  u = int(u)\n",
        "  i = int(i)\n",
        "  user_item_sequence_dict[u].append(i)\n",
        "  if u > u_max:\n",
        "    u_max = u\n",
        "  if i > i_max:\n",
        "    i_max = i\n",
        "\n",
        "user_seq = {}\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "for u in user_item_sequence_dict:\n",
        "  seq = user_item_sequence_dict[u]\n",
        "  l = len(seq)\n",
        "  user_valid[u] = []\n",
        "  user_test[u] = []\n",
        "  if l < 3:\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "  elif l < config.n:\n",
        "    # padding 필요\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2] # n - 1번째\n",
        "    user_test[u] = padded_seq[-1] # n번째\n",
        "  else:\n",
        "    # l > config.n\n",
        "    padded_seq = seq[-config.n:]\n",
        "    user_train[u] = padded_seq[:-2]\n",
        "    user_valid[u] = padded_seq[-2]\n",
        "    user_test[u] = padded_seq[-1]\n",
        "  user_seq[u] = padded_seq\n",
        "\n",
        "print(\"u_max\", u_max)\n",
        "print(\"i_max\", i_max)\n",
        "print(len(padded_seq))"
      ],
      "metadata": {
        "id": "uxiK7K70izuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8f4382-e706-42e9-b808-a36d6166cb97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_max 334730\n",
            "i_max 13047\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head"
      ],
      "metadata": {
        "id": "RO8eEmvVYQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, kdim = None, vdim = None):\n",
        "    \"\"\"\n",
        "\n",
        "    embed_dim: Dimensionality of the input and output embeddings.\n",
        "    num_heads: Number of attention heads.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.kdim = kdim if kdim is not None else embed_dim\n",
        "    self.vdim = vdim if vdim is not None else embed_dim\n",
        "\n",
        "    # 그냥 Q, K, V 사이즈 통일\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.kdim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.vdim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    # nn.init.uniform_(self.q_proj.bias)\n",
        "    # nn.init.uniform_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.q_proj.bias)\n",
        "    nn.init.zeros_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.v_proj.bias)\n",
        "    nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Head 늘리는 방식 중에 embedding 사이즈 줄여서 head수 늘리는 방식\n",
        "\n",
        "    x : (batch_size, -1, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "      (batch_size, num_heads, -1, embed_dim // num_heads).\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = x.shape[0]\n",
        "    splited = x.reshape(n_batch, -1, self.num_heads, self.embed_dim // self.num_heads)\n",
        "\n",
        "    return splited.transpose(1, 2) # (B, num_heads, -1, embed_dim // num_heads)\n",
        "\n",
        "  def scaled_dot_product_attention(self, wq, wk, wv, pad_mask = None):\n",
        "    \"\"\"\n",
        "\n",
        "    wq, wk, wv: (B, num_heads, n_seq, embed_dim // num_heads).\n",
        "\n",
        "    Returns:\n",
        "      Scaled Dot-Product Attention  (B, n_seq, embed_dim).\n",
        "      Average attention weights across heads (B, n_seq, n_key) -시각화에 사용\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_batch = wq.shape[0]\n",
        "    d_k = self.embed_dim // self.num_heads\n",
        "\n",
        "    \"\"\"\n",
        "    wq @ wk.T: (B, num_heads, n_seq, n_key)\n",
        "    pad_mask: (B, n_key)\n",
        "    softmax @ v: (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    \"\"\"\n",
        "    print(\"wq (B, num_heads, n_seq, embed_dim)\", wq.shape)\n",
        "    wk_t = wk.transpose(2, 3) # (B, num_heads, embed_dim // num_heads, n_key)\n",
        "    print(\"wk_t\", wk_t.shape, \"wq\", wq.shape)\n",
        "    similarity = torch.matmul(wq, wk_t) # (B, num_heads, n_seq, n_key)\n",
        "    similarity /= math.sqrt(d_k)\n",
        "    epsilon = -1e9\n",
        "\n",
        "    print(\"similarity\", similarity.shape)\n",
        "\n",
        "    if pad_mask is not None:\n",
        "      similarity = similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), epsilon) # (B, 1, 1, n_key)\n",
        "    print(\"similarity\", similarity.shape)\n",
        "    softmax_sim = torch.softmax(similarity, dim = 3) # key마다 얼마나 비슷 -> softmax (B, num_heads, n_seq, n_key)\n",
        "    average_att_w = torch.mean(softmax_sim, dim = 1) # head 마다 (B, n_seq, n_key) 평균\n",
        "\n",
        "    # average_att_w 찍어보면 각 key 마다의 활성화정도 시각화 가능\n",
        "\n",
        "    attention_value = torch.matmul(softmax_sim, wv) # (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    attention_value = attention_value.transpose(1, 2) # (B, n_key, num_heads, embed_dim // num_heads)\n",
        "    # 사이즈 원상 복구 근데 이게 맞나??? 좀 이상함,,\n",
        "    attention_value = attention_value.reshape(n_batch, -1, self.embed_dim) # (B, n_key, embed_dim)\n",
        "    print(\"attention_value\", attention_value.shape)\n",
        "    return attention_value, average_att_w\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    \"\"\"\n",
        "    q: (B, n_seq, embed_dim)\n",
        "    k: (B, n_key, kdim)\n",
        "    v: (B, n_key, vdim)\n",
        "    pad_mask: (B, n_key)\n",
        "\n",
        "    Returns:\n",
        "      output (B, -1, embed_dim)\n",
        "      average_att_w: (B, n_seq, n_key)\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Multihead input q\", q.shape)\n",
        "    wq = self.q_proj(q) #(B, n_seq, embed_dim)\n",
        "    wk = self.k_proj(k) #(B, n_key, kdim)\n",
        "    wv = self.v_proj(v) # #(B, n_key, embed_dim)\n",
        "\n",
        "    wk = self.split_heads(wk)\n",
        "    wq = self.split_heads(wq)\n",
        "    wv = self.split_heads(wv)\n",
        "\n",
        "    x, attn = self.scaled_dot_product_attention(wq, wk, wv, pad_mask)\n",
        "    x = self.out_proj(x) # (B, n_key, embed_dim)\n",
        "    print(\"x, attn\", x.shape, attn.shape)\n",
        "    return x, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  '''\n",
        "  RELU(SW(1)+b(1))W(2) + b(2)\n",
        "  W(1) & W(2) : (d, d)\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_dim, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, embedding_dim) #(d, d)\n",
        "    self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_ = self.linear1(x_)\n",
        "    x_ = self.dropout(x_) # 이게 중간에 들어가는지 의문,,\n",
        "    x_ = self.relu(x_)\n",
        "    x_ = self.linear2(x_)\n",
        "    x_ = x + self.dropout(x_)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "zofos4MYYQHe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([[False, True], [True, False]])\n",
        "print(pad_mask.shape)\n",
        "similarity = torch.tensor([[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]) # (2, 2, 1, 2)\n",
        "print(similarity.shape)\n",
        "print(similarity)\n",
        "s =similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), 1e-9) # (B, 1, 1, n_key)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M73g9OWcd4R",
        "outputId": "36d35a34-cfa3-496d-f5c7-d00d7f47d3ee"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[1, 2]],\n",
            "\n",
            "         [[3, 4]]],\n",
            "\n",
            "\n",
            "        [[[5, 6]],\n",
            "\n",
            "         [[7, 8]]]])\n",
            "tensor([[[[1, 0]],\n",
            "\n",
            "         [[3, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 6]],\n",
            "\n",
            "         [[0, 8]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "ju7YwMNFoQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_heads, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads)\n",
        "    self.feedforward = FeedForward(embedding_dim)\n",
        "    self.layernorm = nn.LayerNorm(embedding_dim, eps = 1e-8)\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    '''\n",
        "    Residual connection\n",
        "    Q, K, V로 모두 같은 값 사용\n",
        "\n",
        "    g(x) = x + Dropout(g(LayerNorm(x)))\n",
        "    '''\n",
        "    # Self attention Layer\n",
        "    print(\"Encoder block input x\", x.shape)\n",
        "    x_ = self.layernorm(x)\n",
        "    res1, attn = self.attention.forward(x_, x_, x_, pad_mask = mask)\n",
        "    print(\"res1\", res1.shape)\n",
        "    res1 = self.dropout(res1)\n",
        "\n",
        "    x = x + res1\n",
        "\n",
        "    # FFN\n",
        "    x_ = self.layernorm(x)\n",
        "    res2 = self.feedforward(x_)\n",
        "    res2 = self.dropout(res2)\n",
        "    x = x + res2\n",
        "\n",
        "    return x, attn"
      ],
      "metadata": {
        "id": "GEeLucgQoRiz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec model"
      ],
      "metadata": {
        "id": "jXfS54OEodFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "u_max =334730\n",
        "i_max =13047\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self, config, user_num = u_max, item_num = i_max):\n",
        "    super(SASRec, self).__init__()\n",
        "\n",
        "    self.user_num = user_num\n",
        "    self.item_num = item_num\n",
        "    self.config = config\n",
        "    self.num_blocks = config.num_blocks\n",
        "    self.embedding_dim = self.config.d\n",
        "    self.num_heads = config.num_heads\n",
        "\n",
        "    print(\"user_num:\", user_num) # maybe batch size\n",
        "    print(\"item_num:\", item_num)\n",
        "    print(\"embedding_dim\", self.embedding_dim)\n",
        "    print(\"num_heads:\", self.num_heads)\n",
        "    print(\"n:\", config.n)\n",
        "\n",
        "    # M : item embedding (N X d) 인데 item index는 1 부터 시작 => + 1해주기\n",
        "    self.M = torch.nn.Embedding(self.item_num + 1, self.config.d, padding_idx = 0) # 패딩은 0으로\n",
        "\n",
        "    self.pos_enc = nn.Parameter(torch.randn(self.config.n, self.config.d)) # learnable PE (n, d)\n",
        "    # nn.Embedding(self.config.n, self.config.d)\n",
        "    self.droptout = nn.Dropout(self.config.dropout)\n",
        "    self.encoders = nn.ModuleList()\n",
        "\n",
        "    for block in range(self.num_blocks):\n",
        "      self.encoders.append(EncoderBlock(self.embedding_dim, self.num_heads))\n",
        "\n",
        "    self.last_layernorm = torch.nn.LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "     x: (n_batch, seq_length = config.n - 2)\n",
        "     valid랑 test용 빠진 애들 들어오고\n",
        "    '''\n",
        "    print(\"x shape\", x.shape)\n",
        "    embedded_seq = self.M(torch.LongTensor(x)) # id는 정수들의 sequence (n_batch, config.n, embedding_dim)\n",
        "    print(\"embedded_seq\", embedded_seq.shape)\n",
        "    embedded_seq = embedded_seq + self.pos_enc\n",
        "    pad_mask = torch.BoolTensor(x == 0)\n",
        "\n",
        "    # embedded_seq = embedded_seq.masked_fill(pad_mask.unsqueeze(-1))\n",
        "\n",
        "    # future item 가리기\n",
        "    attention_mask = ~torch.tril(torch.ones((x.shape[1], x.shape[1]), dtype = torch.bool))\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      embedded_seq = embedded_seq.masked_fill(pad_mask.unsqueeze(-1), 0)\n",
        "      embedded_seq, attn = encoder(embedded_seq, attention_mask)\n",
        "\n",
        "    output = embedded_seq # (B, n_key, embed_dim)\n",
        "    print(\"output\", output.shape)\n",
        "    # positive sequence : left shifted\n",
        "    seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    pos_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "    neg_seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "\n",
        "    for u in range(x.shape[0]):\n",
        "      pos_seq[2:] = x[u][:-2] # config.n\n",
        "\n",
        "    pos_embs = self.M(torch.LongTensor(pos_seq))\n",
        "    neg_embs = self.M(torch.LongTensor(neg_seq))\n"
      ],
      "metadata": {
        "id": "ugJIaNy61Sg6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 6\n",
        "num_heads = 3\n",
        "kdim = 3\n",
        "vdim = 3\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "my_attention = MultiheadAttention(\n",
        "    embed_dim = embed_dim,\n",
        "    num_heads = num_heads,\n",
        "    kdim = kdim,\n",
        "    vdim = vdim )\n",
        "\n",
        "print(my_attention)\n",
        "\n",
        "my_config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 5, # 300 for ml-1m\n",
        "    d = 3,\n",
        "    batch_size = 2,\n",
        "    test_batch_size = 1,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 20,\n",
        "    patience = 20,\n",
        "    num_heads =3\n",
        "\n",
        ")\n",
        "my_sasrec = SASRec(\n",
        "    my_config\n",
        ")\n",
        "us = [1, 2, 3, 4, 5]\n",
        "my_sasrec.forward(torch.tensor([us]))\n"
      ],
      "metadata": {
        "id": "v9kdiXmFJ-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "783377e6-a5ea-486b-cbd1-bf8007839a27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiheadAttention(\n",
            "  (q_proj): Linear(in_features=6, out_features=6, bias=True)\n",
            "  (k_proj): Linear(in_features=3, out_features=6, bias=True)\n",
            "  (v_proj): Linear(in_features=3, out_features=6, bias=True)\n",
            "  (out_proj): Linear(in_features=6, out_features=6, bias=True)\n",
            ")\n",
            "user_num: 334730\n",
            "item_num: 13047\n",
            "embedding_dim 3\n",
            "num_heads: 3\n",
            "n: 5\n",
            "x shape torch.Size([1, 5])\n",
            "embedded_seq torch.Size([1, 5, 3])\n",
            "Encoder block input x torch.Size([1, 5, 3])\n",
            "Multihead input q torch.Size([1, 5, 3])\n",
            "wq (B, num_heads, n_seq, embed_dim) torch.Size([1, 3, 5, 1])\n",
            "wk_t torch.Size([1, 3, 1, 5]) wq torch.Size([1, 3, 5, 1])\n",
            "similarity torch.Size([1, 3, 5, 5])\n",
            "similarity torch.Size([5, 3, 5, 5])\n",
            "attention_value torch.Size([1, 25, 3])\n",
            "x, attn torch.Size([1, 25, 3]) torch.Size([5, 5, 5])\n",
            "res1 torch.Size([1, 25, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (5) must match the size of tensor b (25) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bfe3092ad78f>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[0mus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmy_sasrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7c985693830d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0membedded_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0membedded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_seq\u001b[0m \u001b[0;31m# (B, n_key, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-53b8212c49b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# FFN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (25) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "kyHk6zAJfMM9",
        "outputId": "6300e551-394d-469c-ae22-343126f1c830"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-640da0ca6e42>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_sasrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-aac60c7a7cf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# future item 가리기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_seq[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HgLdtqzfqFc",
        "outputId": "de99f88e-a46c-4e90-a449-f4e101352ca2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2570, 3240, 3634, 3838, 4178, 4688, 4895, 5030, 5185, 5249, 5251, 5762, 6166, 6589, 6775, 6853, 7151, 7203, 7225, 7262, 8326, 8432, 9074, 9594, 9697, 9793, 9971, 9975, 10361, 10367, 10789, 11025, 11102, 11292, 11349, 11407, 11535, 11771, 12047, 12222, 12483, 12891, 12920, 1563, 2832, 4230, 4528, 9584, 10084, 8112]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-pElBdff8Eq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}