{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "etC2-FZjeyHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ef267d-63d6-4f08-bc8b-5c392d2d78ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/RecommanderSystems')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/RecommanderSystems')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    seed = 1,\n",
        "    data = \"Steam.txt\", # Beauty.txt / ml-1m.txt\n",
        "    dropout = 0.5, # 0.2 for ml-1m\n",
        "    n = 50, # 300 for ml-1m, 50 for others\n",
        "    d = 40,\n",
        "    batch_size = 128,\n",
        "    num_heads = 1, # default\n",
        "    test_batch_size = 100,\n",
        "    weight_decay = 5e-4,\n",
        "    num_blocks = 2,\n",
        "    lr = 1e-3,\n",
        "    epoch = 10,\n",
        "    patience = 20\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "--u0DvNahuJl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "SASRec의 경우 학습을 시킬 때 user 당 item의 시퀀스가 필요"
      ],
      "metadata": {
        "id": "mb6FLTlPtzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_item_sequence_dict = defaultdict(list)\n",
        "\n",
        "u_max = -1\n",
        "i_max = -1\n",
        "\n",
        "data = open(config.data, 'r')\n",
        "for line in data:\n",
        "  u, i = line.strip().split(' ') # (user, item) 조합의 txt 파일 읽기\n",
        "  u = int(u)\n",
        "  i = int(i)\n",
        "  user_item_sequence_dict[u].append(i)\n",
        "  if u > u_max:\n",
        "    u_max = u\n",
        "  if i > i_max:\n",
        "    i_max = i\n",
        "\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "for u in user_item_sequence_dict:\n",
        "  seq = user_item_sequence_dict[u]\n",
        "  l = len(seq)\n",
        "  user_valid[u] = []\n",
        "  user_test[u] = []\n",
        "  if l < 3:\n",
        "    padded_seq = [0] * (config.n - l) + seq\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "  elif l >= 52:\n",
        "    user_train[u] = seq[-config.n - 2:-2]\n",
        "    user_valid[u] = seq[-2]\n",
        "    user_test[u] = seq[-1]\n",
        "\n",
        "  else:\n",
        "    user_valid[u] = seq[-2]\n",
        "    user_test[u] = seq[-1]\n",
        "    padded_seq = [0] * (config.n - l + 2) + seq[:-2]\n",
        "    user_train[u] = padded_seq\n",
        "\n",
        "print(\"u_max\", u_max)\n",
        "print(\"i_max\", i_max)\n",
        "print(len(user_train[1]))"
      ],
      "metadata": {
        "id": "uxiK7K70izuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32889ce-99fc-4bea-eaf5-649f4387efbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_max 334730\n",
            "i_max 13047\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head"
      ],
      "metadata": {
        "id": "RO8eEmvVYQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, kdim = None, vdim = None):\n",
        "    \"\"\"\n",
        "\n",
        "    embed_dim: Dimensionality of the input and output embeddings.\n",
        "    num_heads: Number of attention heads.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.kdim = kdim if kdim is not None else embed_dim\n",
        "    self.vdim = vdim if vdim is not None else embed_dim\n",
        "\n",
        "    # 그냥 Q, K, V 사이즈 통일\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.kdim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.vdim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    # nn.init.uniform_(self.q_proj.bias)\n",
        "    # nn.init.uniform_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.q_proj.bias)\n",
        "    nn.init.zeros_(self.k_proj.bias)\n",
        "    nn.init.zeros_(self.v_proj.bias)\n",
        "    nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Head 늘리는 방식 중에 embedding 사이즈 줄여서 head수 늘리는 방식\n",
        "\n",
        "    x : (batch_size, -1, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "      (batch_size, num_heads, -1, embed_dim // num_heads).\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = x.shape[0]\n",
        "    splited = x.reshape(n_batch, -1, self.num_heads, self.embed_dim // self.num_heads)\n",
        "\n",
        "    return splited.transpose(1, 2) # (B, num_heads, -1, embed_dim // num_heads)\n",
        "\n",
        "  def scaled_dot_product_attention(self, wq, wk, wv, pad_mask = None):\n",
        "    \"\"\"\n",
        "\n",
        "    wq, wk, wv: (B, num_heads, n_seq, embed_dim // num_heads).\n",
        "\n",
        "    Returns:\n",
        "      Scaled Dot-Product Attention  (B, n_seq, embed_dim).\n",
        "      Average attention weights across heads (B, n_seq, n_key) -시각화에 사용\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_batch = wq.shape[0]\n",
        "    d_k = self.embed_dim // self.num_heads\n",
        "\n",
        "    \"\"\"\n",
        "    wq @ wk.T: (B, num_heads, n_seq, n_key)\n",
        "    pad_mask: (B, n_key)\n",
        "    softmax @ v: (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    \"\"\"\n",
        "\n",
        "    wk_t = wk.transpose(2, 3) # (B, num_heads, embed_dim // num_heads, n_key)\n",
        "\n",
        "    similarity = torch.matmul(wq, wk_t) # (B, num_heads, n_seq, n_key)\n",
        "    similarity /= math.sqrt(d_k)\n",
        "    epsilon = -1e9\n",
        "\n",
        "    # pad_mask가 여기서는 tril로 들어와서 (n, n)으로 들어옴 => (1, 1, n, n)으로 unsqueeze\n",
        "    if pad_mask is not None:\n",
        "      similarity = similarity.masked_fill(pad_mask.unsqueeze(0).unsqueeze(1), epsilon) # (B, 1, 1, n_key)\n",
        "\n",
        "    softmax_sim = torch.softmax(similarity, dim = 3) # key마다 얼마나 비슷 -> softmax (B, num_heads, n_seq, n_key)\n",
        "    average_att_w = torch.mean(softmax_sim, dim = 1) # head 마다 (B, n_seq, n_key) 평균\n",
        "\n",
        "    # average_att_w 찍어보면 각 key 마다의 활성화정도 시각화 가능\n",
        "\n",
        "    attention_value = torch.matmul(softmax_sim, wv) # (B, num_heads, n_seq, embed_dim // num_heads)\n",
        "    attention_value = attention_value.transpose(1, 2) # (B, n_key, num_heads, embed_dim // num_heads)\n",
        "    # 사이즈 원상 복구 근데 이게 맞나??? 좀 이상함,,\n",
        "    attention_value = attention_value.reshape(n_batch, -1, self.embed_dim) # (B, n_key, embed_dim)\n",
        "\n",
        "    return attention_value, average_att_w\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    \"\"\"\n",
        "    q: (B, n_seq, embed_dim)\n",
        "    k: (B, n_key, kdim)\n",
        "    v: (B, n_key, vdim)\n",
        "    pad_mask: (B, n_key)\n",
        "\n",
        "    Returns:\n",
        "      output (B, -1, embed_dim)\n",
        "      average_att_w: (B, n_seq, n_key)\n",
        "\n",
        "    \"\"\"\n",
        "    # print(\"Multihead input q\", q.shape)\n",
        "    wq = self.q_proj(q) #(B, n_seq, embed_dim)\n",
        "    wk = self.k_proj(k) #(B, n_key, kdim)\n",
        "    wv = self.v_proj(v) # #(B, n_key, embed_dim)\n",
        "\n",
        "    wk = self.split_heads(wk)\n",
        "    wq = self.split_heads(wq)\n",
        "    wv = self.split_heads(wv)\n",
        "\n",
        "    x, attn = self.scaled_dot_product_attention(wq, wk, wv, pad_mask)\n",
        "    x = self.out_proj(x) # (B, n_key, embed_dim)\n",
        "\n",
        "    return x, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  '''\n",
        "  RELU(SW(1)+b(1))W(2) + b(2)\n",
        "  W(1) & W(2) : (d, d)\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_dim, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, embedding_dim) #(d, d)\n",
        "    self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_ = self.linear1(x)\n",
        "    x_ = self.dropout(x_) # 이게 중간에 들어가는지 의문,,\n",
        "    x_ = self.relu(x_)\n",
        "    x_ = self.linear2(x_)\n",
        "    x_ = x + self.dropout(x_)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "zofos4MYYQHe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([[False, True], [True, False]]) # (2, 2)\n",
        "print(pad_mask.shape)\n",
        "similarity = torch.tensor([[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]) # (2, 2, 1, 2)\n",
        "print(similarity.shape)\n",
        "print(similarity)\n",
        "s =similarity.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), 1e-9) # (B, 1, 1, n_key)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M73g9OWcd4R",
        "outputId": "4294ff58-b48c-418b-8376-a0160fe2a6d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[1, 2]],\n",
            "\n",
            "         [[3, 4]]],\n",
            "\n",
            "\n",
            "        [[[5, 6]],\n",
            "\n",
            "         [[7, 8]]]])\n",
            "tensor([[[[1, 0]],\n",
            "\n",
            "         [[3, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 6]],\n",
            "\n",
            "         [[0, 8]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_mask = torch.tensor([False, True]) # (2)\n",
        "a = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2, 3)\n",
        "p = pad_mask.unsqueeze(-1)\n",
        "print(p)\n",
        "print(a.masked_fill(p, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lIBTTNe9ho",
        "outputId": "70702527-f969-4e9c-c3ae-fd7284d898f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [ True]])\n",
            "tensor([[1, 2, 3],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "ju7YwMNFoQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_heads, dropout_rate = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads)\n",
        "    self.feedforward = FeedForward(embedding_dim)\n",
        "    self.layernorm = nn.LayerNorm(embedding_dim, eps = 1e-8)\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    '''\n",
        "    Residual connection\n",
        "    Q, K, V로 모두 같은 값 사용\n",
        "\n",
        "    g(x) = x + Dropout(g(LayerNorm(x)))\n",
        "    '''\n",
        "    # Self attention Layer\n",
        "\n",
        "    x_ = self.layernorm(x)\n",
        "    res1, attn = self.attention.forward(x_, x_, x_, pad_mask = mask)\n",
        "    res1 = self.dropout(res1)\n",
        "\n",
        "    x = x + res1\n",
        "\n",
        "    # FFN\n",
        "    x_ = self.layernorm(x)\n",
        "    res2 = self.feedforward(x_)\n",
        "    res2 = self.dropout(res2)\n",
        "    x = x + res2\n",
        "\n",
        "    return x, attn"
      ],
      "metadata": {
        "id": "GEeLucgQoRiz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "xw4ZcLi9RCpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class UserTrainDataset(Dataset):\n",
        "  def __init__(self, user_train):\n",
        "    self.user_train = user_train\n",
        "\n",
        "    self.users = list(user_train.keys())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    user_id = self.users[idx]\n",
        "    user_data = self.user_train[user_id]\n",
        "\n",
        "    return user_id, user_data\n",
        "\n",
        "def collate_fn(batch):\n",
        "  user_ids, user_data = zip(*batch)\n",
        "  user_data_tensor = torch.tensor(user_data, dtype=torch.long)\n",
        "  return user_ids, user_data_tensor\n",
        "\n",
        "user_train_dataset = UserTrainDataset(user_train)\n",
        "user_train_dataloader = DataLoader(user_train_dataset, batch_size= config.batch_size, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "qdf6c3ynRBnr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec model"
      ],
      "metadata": {
        "id": "jXfS54OEodFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# user_train_dataset = UserTrainDataset(user_train)\n",
        "# u_max =5\n",
        "# i_max =10\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self, config, user_train_dataloader = user_train_dataloader, user_train = user_train, user_num = u_max, item_num = i_max):\n",
        "    super(SASRec, self).__init__()\n",
        "\n",
        "    self.user_num = user_num\n",
        "    self.item_num = item_num\n",
        "    self.config = config\n",
        "    self.num_blocks = config.num_blocks\n",
        "    self.embedding_dim = self.config.d\n",
        "    self.num_heads = config.num_heads\n",
        "    self.user_seq_dataloader = user_train_dataloader\n",
        "    self.user_train = user_train\n",
        "\n",
        "    print(\"user_num:\", user_num) # maybe batch size\n",
        "    print(\"item_num:\", item_num)\n",
        "    print(\"embedding_dim\", self.embedding_dim)\n",
        "    print(\"num_heads:\", self.num_heads)\n",
        "    print(\"n:\", config.n)\n",
        "\n",
        "    # M : item embedding (N X d) 인데 item index는 1 부터 시작 => + 1해주기\n",
        "    self.M = torch.nn.Embedding(self.item_num + 1, self.config.d, padding_idx = 0) # 패딩은 0으로\n",
        "\n",
        "    self.pos_enc = nn.Parameter(torch.randn(self.config.n, self.config.d)) # learnable PE (n, d)\n",
        "\n",
        "    self.droptout = nn.Dropout(self.config.dropout)\n",
        "    self.encoders = nn.ModuleList()\n",
        "\n",
        "    for block in range(self.num_blocks):\n",
        "      self.encoders.append(EncoderBlock(self.embedding_dim, self.num_heads))\n",
        "\n",
        "    self.last_layernorm = torch.nn.LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "    self.criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(), lr=config.lr)\n",
        "\n",
        "  def forward(self, user_ids, x):\n",
        "    '''\n",
        "    x: (n_batch, config.n)\n",
        "    user_train[idx]\n",
        "    '''\n",
        "    # print(\"x\", x.shape)\n",
        "    embedded_seq = self.M(torch.LongTensor(x)) # id는 정수들의 sequence (n_batch, config.n, embedding_dim)\n",
        "    embedded_seq = embedded_seq + self.pos_enc\n",
        "    pad_mask = torch.BoolTensor(x == 0).unsqueeze(-1)\n",
        "    # future item 가리기\n",
        "    attention_mask = ~torch.tril(torch.ones((x.shape[1], x.shape[1]), dtype = torch.bool))\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      embedded_seq = embedded_seq.masked_fill(pad_mask, 0)\n",
        "      embedded_seq, attn = encoder(embedded_seq, attention_mask)\n",
        "\n",
        "    output = self.last_layernorm(embedded_seq) # (B, n_key, embed_dim)\n",
        "    # print(\"output\", output.shape)\n",
        "    l = 0\n",
        "    rot_list = []\n",
        "    rjt_list = []\n",
        "    seq_l = []\n",
        "    pos = []\n",
        "    neg = []\n",
        "\n",
        "    for l, u in enumerate(user_ids):\n",
        "      seq = np.zeros([self.config.n], dtype = np.int32)\n",
        "      pos_seq = np.zeros([self.config.n], dtype = np.int32) # positive sequence : left shifted\n",
        "      neg_seq = np.zeros([self.config.n], dtype = np.int32) # Random negative\n",
        "      answer = x[l][-1]\n",
        "      idx = self.config.n - 1\n",
        "\n",
        "      for item in reversed(x[l]):\n",
        "        seq[idx] = item\n",
        "        pos_seq[idx] = answer\n",
        "        if answer != 0:\n",
        "          ns = np.random.randint(1, self.item_num + 1)\n",
        "          while ns in self.user_train[u]:\n",
        "            ns = np.random.randint(1, self.item_num + 1)\n",
        "          neg_seq[idx] = ns\n",
        "          answer = item\n",
        "          if idx == 0:\n",
        "            break\n",
        "          idx -= 1\n",
        "\n",
        "      seq_l.append(torch.LongTensor(seq))\n",
        "      pos.append(torch.LongTensor(pos_seq))\n",
        "      neg.append(torch.LongTensor(neg_seq))\n",
        "\n",
        "\n",
        "    seq_l = torch.stack(seq_l)\n",
        "    pos_embs = self.M(torch.LongTensor(torch.stack(pos)))\n",
        "    neg_embs = self.M(torch.LongTensor(torch.stack(neg)))\n",
        "    rot = (output * pos_embs).sum(dim = - 1)\n",
        "    rjt = (output * neg_embs).sum(dim = - 1)\n",
        "\n",
        "    indicies = np.where( seq_l != 0)\n",
        "\n",
        "    return rot, rjt, indicies\n",
        "\n",
        "  def train(self):\n",
        "    '''\n",
        "     batch: (n_batch, seq_length = config.n - 2)\n",
        "     user_seq -2 / -1 / :-2 valid / test / train\n",
        "     '''\n",
        "    for epoch in range(self.config.epoch):\n",
        "      batch_num = 0\n",
        "      for idx, batch in self.user_seq_dataloader:\n",
        "        rot, rjt, indices = self.forward(idx, batch)\n",
        "        pos_labels, neg_labels = torch.ones(rot.shape), torch.zeros(rjt.shape)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(rot[indices], pos_labels[indices]) + self.criterion(rjt[indices], neg_labels[indices])\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if batch_num % 500 == 0:\n",
        "          print(\"Epoch\", epoch, \"Batch_num\", batch_num, \"loss\", loss.item())\n",
        "        batch_num += 1\n",
        "      print(\"loss in epoch {}: {}\".format(epoch,loss.item()))\n",
        "      torch.save(self.state_dict(), 'sasrec_model.pth')\n",
        "\n",
        "  def predict(self):\n",
        "    '''\n",
        "    predict next item\n",
        "    '''\n",
        "\n"
      ],
      "metadata": {
        "id": "ugJIaNy61Sg6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SASRec(config)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "5HgLdtqzfqFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = SASRec(config)\n",
        "loaded_model.load_state_dict(torch.load('sasrec_model.pth'))\n",
        "loaded_model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jydjLiUUNRyY",
        "outputId": "bb6d9ecc-490a-4fbd-983b-40f310828b76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_num: 334730\n",
            "item_num: 13047\n",
            "embedding_dim 40\n",
            "num_heads: 1\n",
            "n: 50\n",
            "Epoch 0 Batch_num 0 loss 0.5637531280517578\n",
            "Epoch 0 Batch_num 500 loss 0.6437857151031494\n",
            "Epoch 0 Batch_num 1000 loss 0.5738688111305237\n",
            "Epoch 0 Batch_num 1500 loss 0.6348236799240112\n",
            "Epoch 0 Batch_num 2000 loss 0.5544074177742004\n",
            "Epoch 0 Batch_num 2500 loss 0.6047042012214661\n",
            "loss in epoch 0: 0.9048413038253784\n",
            "Epoch 1 Batch_num 0 loss 0.518990159034729\n",
            "Epoch 1 Batch_num 500 loss 0.601277232170105\n",
            "Epoch 1 Batch_num 1000 loss 0.6141561269760132\n",
            "Epoch 1 Batch_num 1500 loss 0.5879354476928711\n",
            "Epoch 1 Batch_num 2000 loss 0.5514355301856995\n",
            "Epoch 1 Batch_num 2500 loss 0.5943735241889954\n",
            "loss in epoch 1: 0.31292665004730225\n",
            "Epoch 2 Batch_num 0 loss 0.6518396139144897\n",
            "Epoch 2 Batch_num 500 loss 0.7028895616531372\n",
            "Epoch 2 Batch_num 1000 loss 0.650926947593689\n",
            "Epoch 2 Batch_num 1500 loss 0.55503910779953\n",
            "Epoch 2 Batch_num 2000 loss 0.5773842930793762\n",
            "Epoch 2 Batch_num 2500 loss 0.5827951431274414\n",
            "loss in epoch 2: 0.4921734929084778\n",
            "Epoch 3 Batch_num 0 loss 0.47662919759750366\n",
            "Epoch 3 Batch_num 500 loss 0.5661110281944275\n",
            "Epoch 3 Batch_num 1000 loss 0.5934500098228455\n",
            "Epoch 3 Batch_num 1500 loss 0.6434738636016846\n",
            "Epoch 3 Batch_num 2000 loss 0.5453523397445679\n",
            "Epoch 3 Batch_num 2500 loss 0.5512625575065613\n",
            "loss in epoch 3: 0.5359491109848022\n",
            "Epoch 4 Batch_num 0 loss 0.573438286781311\n",
            "Epoch 4 Batch_num 500 loss 0.5814409852027893\n",
            "Epoch 4 Batch_num 1000 loss 0.6123870015144348\n",
            "Epoch 4 Batch_num 1500 loss 0.5462672710418701\n",
            "Epoch 4 Batch_num 2000 loss 0.562442421913147\n",
            "Epoch 4 Batch_num 2500 loss 0.6686998605728149\n",
            "loss in epoch 4: 0.5990748405456543\n",
            "Epoch 5 Batch_num 0 loss 0.5670853853225708\n",
            "Epoch 5 Batch_num 500 loss 0.513556718826294\n",
            "Epoch 5 Batch_num 1000 loss 0.5782581567764282\n",
            "Epoch 5 Batch_num 1500 loss 0.5677189826965332\n",
            "Epoch 5 Batch_num 2000 loss 0.4992718994617462\n",
            "Epoch 5 Batch_num 2500 loss 0.5529924631118774\n",
            "loss in epoch 5: 0.7148087024688721\n",
            "Epoch 6 Batch_num 0 loss 0.5561720132827759\n",
            "Epoch 6 Batch_num 500 loss 0.504864513874054\n",
            "Epoch 6 Batch_num 1000 loss 0.5751625895500183\n",
            "Epoch 6 Batch_num 1500 loss 0.5639245510101318\n",
            "Epoch 6 Batch_num 2000 loss 0.49282628297805786\n",
            "Epoch 6 Batch_num 2500 loss 0.47179341316223145\n",
            "loss in epoch 6: 0.5770288705825806\n",
            "Epoch 7 Batch_num 0 loss 0.5837289690971375\n",
            "Epoch 7 Batch_num 500 loss 0.5387847423553467\n",
            "Epoch 7 Batch_num 1000 loss 0.5179708003997803\n",
            "Epoch 7 Batch_num 1500 loss 0.5172352194786072\n",
            "Epoch 7 Batch_num 2000 loss 0.5313659906387329\n",
            "Epoch 7 Batch_num 2500 loss 0.5730477571487427\n",
            "loss in epoch 7: 0.3749731183052063\n",
            "Epoch 8 Batch_num 0 loss 0.5915217399597168\n",
            "Epoch 8 Batch_num 500 loss 0.54185950756073\n",
            "Epoch 8 Batch_num 1000 loss 0.5009508728981018\n",
            "Epoch 8 Batch_num 1500 loss 0.5567046403884888\n",
            "Epoch 8 Batch_num 2000 loss 0.5295748710632324\n",
            "Epoch 8 Batch_num 2500 loss 0.5259437561035156\n",
            "loss in epoch 8: 0.42135006189346313\n",
            "Epoch 9 Batch_num 0 loss 0.5303215980529785\n",
            "Epoch 9 Batch_num 500 loss 0.5867708921432495\n",
            "Epoch 9 Batch_num 1000 loss 0.4579625129699707\n",
            "Epoch 9 Batch_num 1500 loss 0.5181784629821777\n",
            "Epoch 9 Batch_num 2000 loss 0.49924755096435547\n",
            "Epoch 9 Batch_num 2500 loss 0.5314056873321533\n",
            "loss in epoch 9: 0.5783854126930237\n"
          ]
        }
      ]
    }
  ]
}